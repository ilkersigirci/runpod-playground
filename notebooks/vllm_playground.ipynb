{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GGUF TEST\n",
    "\n",
    "```bash\n",
    "# Download the model\n",
    "huggingface-cli download $DEPLOYED_MODEL_NAME --repo-type model --revision main --local-dir $LIBRARY_BASE_PATH/models/$SERVED_MODEL_NAME --include \"Codestral-22B-v0.1-Q8_0.gguf\"\n",
    "huggingface-cli download $DEPLOYED_MODEL_NAME --repo-type model --revision main --local-dir $LIBRARY_BASE_PATH/models/$SERVED_MODEL_NAME --include \"Codestral-22B-v0.1.imatrix\"\n",
    "\n",
    "\n",
    "# Start the server\n",
    "SERVED_MODEL_NAME=\"${DEPLOYED_MODEL_NAME#*/}\"\n",
    "MODEL_PATH=$LIBRARY_BASE_PATH/models/$SERVED_MODEL_NAME/Codestral-22B-v0.1-Q8_0.gguf\n",
    "GPU_COUNT=$(nvidia-smi --query-gpu=count --format=csv,noheader,nounits | head -n 1)\n",
    "\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --enable-prefix-caching \\\n",
    "    --gpu-memory-utilization 0.97 \\\n",
    "    --tensor-parallel-size $GPU_COUNT \\\n",
    "    --max-model-len $MAX_CONTEXT_LEN \\\n",
    "    --model $MODEL_PATH \\\n",
    "    --served-model-name $SERVED_MODEL_NAME \\\n",
    "    --quantization gguf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "def run_gguf_inference(model_path):\n",
    "    llm = LLM(\n",
    "\tmodel=model_path,\n",
    "\tmax_model_len=4096,\n",
    "\ttokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "\ttensor_parallel_size=1, \n",
    "    )\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    conversations = tokenizer.apply_chat_template(\n",
    "        [{'role': 'user', 'content': 'what is the future of AI?'}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    outputs = llm.generate(\n",
    "        [conversations],\n",
    "        SamplingParams(temperature=0, max_tokens=1000),\n",
    "    )\n",
    "    for output in outputs:\n",
    "\tprint(output)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    repo_id = \"bullerwins/Meta-Llama-3.1-8B-Instruct-GGUF\"\n",
    "    filename = \"Meta-Llama-3.1-8B-Instruct-Q2_K.gguf\"\n",
    "    model = hf_hub_download(repo_id, filename=filename)\n",
    "    run_gguf_inference(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
