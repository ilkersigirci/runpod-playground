{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GGUF TEST\n",
    "\n",
    "```bash\n",
    "# Download the model\n",
    "huggingface-cli download $HF_MODEL_NAME --repo-type model --revision main --local-dir $LIBRARY_BASE_PATH/models/$SERVED_MODEL_NAME --include \"Codestral-22B-v0.1-Q8_0.gguf\"\n",
    "huggingface-cli download $HF_MODEL_NAME --repo-type model --revision main --local-dir $LIBRARY_BASE_PATH/models/$SERVED_MODEL_NAME --include \"Codestral-22B-v0.1.imatrix\"\n",
    "\n",
    "\n",
    "# Start the server\n",
    "SERVED_MODEL_NAME=\"${HF_MODEL_NAME#*/}\"\n",
    "MODEL_PATH=$LIBRARY_BASE_PATH/models/$SERVED_MODEL_NAME/Codestral-22B-v0.1-Q8_0.gguf\n",
    "GPU_COUNT=$(nvidia-smi --query-gpu=count --format=csv,noheader,nounits | head -n 1)\n",
    "\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --enable-prefix-caching \\\n",
    "    --gpu-memory-utilization 0.97 \\\n",
    "    --tensor-parallel-size $GPU_COUNT \\\n",
    "    --max-model-len $MAX_CONTEXT_LEN \\\n",
    "    --model $MODEL_PATH \\\n",
    "    --served-model-name $SERVED_MODEL_NAME \\\n",
    "    --num-scheduler-steps 8 \\\n",
    "    --use-v2-block-manager \\\n",
    "    --disable-log-stats \\\n",
    "    --chat-template $LIBRARY_BASE_PATH/tabbyAPI/prompt_templates/codestral_gguf.jinja\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "def run_gguf_inference(model_path):\n",
    "    llm = LLM(\n",
    "\tmodel=model_path,\n",
    "\tmax_model_len=4096,\n",
    "\ttokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "\ttensor_parallel_size=1, \n",
    "    )\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    conversations = tokenizer.apply_chat_template(\n",
    "        [{'role': 'user', 'content': 'what is the future of AI?'}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    outputs = llm.generate(\n",
    "        [conversations],\n",
    "        SamplingParams(temperature=0, max_tokens=1000),\n",
    "    )\n",
    "    for output in outputs:\n",
    "\tprint(output)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    repo_id = \"bullerwins/Meta-Llama-3.1-8B-Instruct-GGUF\"\n",
    "    filename = \"Meta-Llama-3.1-8B-Instruct-Q2_K.gguf\"\n",
    "    model = hf_hub_download(repo_id, filename=filename)\n",
    "    run_gguf_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "curl --request POST \\\n",
    "    --url https://1x0y86v7sz5g5d-8000.proxy.runpod.net/v1/chat/completions \\\n",
    "    --header \"Content-Type: application/json\" \\\n",
    "    --data '{\n",
    "  \"model\": \"Codestral-22B-v0.1-GGUF\",\n",
    "  \"messages\": [\n",
    "  {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are a helpful virtual coder assistant trained by OpenAI.\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Implement fibonacci series in python.\"\n",
    "  }\n",
    "  ], \n",
    "  \"temperature\": 0.8,\n",
    "  \"stream\": false\n",
    "}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
